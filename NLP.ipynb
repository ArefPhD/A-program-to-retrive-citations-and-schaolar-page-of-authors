{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Tutorial (v1.8.1): Training, Saving, Loading and Testing",
      "provenance": [],
      "collapsed_sections": [
        "6ABe4kS-XZqa"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArefPhD/A-program-to-retrive-citations-and-schaolar-page-of-authors/blob/master/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmZ1uZecv7qW"
      },
      "source": [
        "#Tutorial (v1.8.1): Training, Saving, Loading and Testing\n",
        "\n",
        "(last updated 12-01-2021)\n",
        "\n",
        "In this tutorial, we are going to use contextualized topic modeling to get topics out of a collections made of Wikipedia Abstracts.\n",
        "\n",
        "## Topic Models \n",
        "\n",
        "Topic models allow you to discover latent topics in your documents in a completely unsuperivsed way. Just use your documents and get topics out.\n",
        "\n",
        "## Contextualized Topic Models\n",
        "\n",
        "![](https://raw.githubusercontent.com/MilaNLProc/contextualized-topic-models/master/img/logo.png)\n",
        "\n",
        "What are Contextualized Topic Models? **CTMs** are a family of topic models that combine the expressive power of BERT embeddings with the usupervised capabilities of topic models to get topics out of documents.\n",
        "\n",
        "## Python Package\n",
        "\n",
        "You can find our package [here](https://github.com/MilaNLProc/contextualized-topic-models).\n",
        "\n",
        "![https://travis-ci.com/MilaNLProc/contextualized-topic-models](https://travis-ci.com/MilaNLProc/contextualized-topic-models.svg) ![https://pypi.python.org/pypi/contextualized_topic_models](https://img.shields.io/pypi/v/contextualized_topic_models.svg) ![https://pepy.tech/badge/contextualized-topic-models](https://pepy.tech/badge/contextualized-topic-models)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln23-soXeQk3"
      },
      "source": [
        "# Enabling the GPU\n",
        "\n",
        "First, you'll need to enable GPUs for the notebook:\n",
        "\n",
        "- Navigate to Edit→Notebook Settings\n",
        "- select GPU from the Hardware Accelerator drop-down\n",
        "\n",
        "[Reference](https://colab.research.google.com/notebooks/gpu.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgxRbgSZ9MsB"
      },
      "source": [
        "# Data\n",
        "\n",
        "We are going to download some abstracts from Wikipedia and use them to run our topic modeling pipeline. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXZ8fOdYwdWO"
      },
      "source": [
        "%%capture\n",
        "!wget https://raw.githubusercontent.com/vinid/data/master/dbpedia_sample_abstract_20k_unprep.txt\n",
        "!wget https://raw.githubusercontent.com/vinid/data/master/dbpedia_sample_abstract_20k_prep.txt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsNRo8I8Yem2"
      },
      "source": [
        "# Installing Contextualized Topic Models\n",
        "\n",
        "Now, we install the contextualized topic model library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUw5EQi8v9r1"
      },
      "source": [
        "%%capture\n",
        "!pip install contextualized-topic-models==1.8.1\n",
        "!pip install torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvlY7e-oW_zU"
      },
      "source": [
        "# Restart the Notebook\n",
        "\n",
        "For the changes to take effect, we now need to restart the notebook.\n",
        "\n",
        "From the Menu:\n",
        "\n",
        "+ Runtime → Restart Runtime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0c_ftxjxY_H"
      },
      "source": [
        "## Importing what we need"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZmTpQUov8y8"
      },
      "source": [
        "from contextualized_topic_models.models.ctm import CombinedTM\n",
        "from contextualized_topic_models.utils.data_preparation import bert_embeddings_from_file, TopicModelDataPreparation\n",
        "from contextualized_topic_models.datasets.dataset import CTMDataset\n",
        "from contextualized_topic_models.evaluation.measures import CoherenceNPMI, InvertedRBO\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from gensim.models import ldamodel \n",
        "import os\n",
        "import numpy as np\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XocA440_YW1h"
      },
      "source": [
        "Let's read our data files and store the documents as lists of strings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDElvKiWYWV3"
      },
      "source": [
        "with open(\"dbpedia_sample_abstract_20k_prep.txt\", 'r') as fr_prep:\n",
        "  text_training_preprocessed = [line.strip() for line in fr_prep.readlines()]\n",
        "\n",
        "with open(\"dbpedia_sample_abstract_20k_unprep.txt\", 'r') as fr_unprep:\n",
        "  text_training_not_preprocessed = [line.strip() for line in fr_unprep.readlines()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mECqdW0Lk53"
      },
      "source": [
        "NOTE: Make sure that the lenghts of the two lists of documents are the same and the index of a not preprocessed document corresponds to the index of the same preprocessed document. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gw7va3D3LP0e"
      },
      "source": [
        "assert len(text_training_preprocessed) == len(text_training_not_preprocessed)\n",
        "\n",
        "print(text_training_not_preprocessed[0])\n",
        "print(text_training_preprocessed[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKO6mcaEsovb"
      },
      "source": [
        "## Let's split the documents in training and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2GYCcydstJW"
      },
      "source": [
        "training_bow_documents = text_training_preprocessed[0:15000]\n",
        "training_contextual_document = text_training_not_preprocessed[0:15000]\n",
        "\n",
        "testing_bow_documents = text_training_preprocessed[15000:]\n",
        "testing_contextual_documents = text_training_not_preprocessed[15000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-AM070Ez6lW"
      },
      "source": [
        "## Creating the Training Dataset\n",
        "Let's pass our files with preprocess data to our TopicModelDataPreparation object. This object takes care of creating the bag of words for you and of obtaining the contextualized BERT representations of documents. This operation allows us to create our training dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhLt6VA3wvCB"
      },
      "source": [
        "tp = TopicModelDataPreparation(\"bert-base-nli-mean-tokens\")\n",
        "\n",
        "training_dataset = tp.create_training_set(training_contextual_document, training_bow_documents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edTXBg2PMlg1"
      },
      "source": [
        "\n",
        "Why do we use the **preprocessed text** here? We need text without punctuation to build the bag of word. Also, we might want only to have the most frequent words inside the BoW. Too many words might not help.\n",
        "\n",
        "And what about the **unpreprocessed text**? We provide unpreprocessed text as the input for BERT (or the contextualized model of your choice) to let the model output more accurate document representations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WQtndP6N8kW"
      },
      "source": [
        "Let's check the vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9I3ThmBf0BcK"
      },
      "source": [
        "tp.vocab[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5agbVqhxdnY"
      },
      "source": [
        "## Training our Combined Contextualized Topic Model\n",
        "\n",
        "Finally, we can fit our new topic model. We will ask the model to find 50 topics in our collection (`n_component` parameter of the CombinedTM object). \n",
        "\n",
        "(Increase the number of epochs if you want to get better results)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dALamw7HxLj-"
      },
      "source": [
        "ctm = CombinedTM(input_size=len(tp.vocab), bert_input_size=768, num_epochs=100, n_components=50)\n",
        "ctm.fit(training_dataset)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-Rvf4r92NWO"
      },
      "source": [
        "### Saving the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GovcXlu-2QUX"
      },
      "source": [
        "ctm.save(models_dir=\"./\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PneLVfb2R0r"
      },
      "source": [
        "### Loading the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bShpxI7a2adS"
      },
      "source": [
        "del ctm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qMC-8CU3NlL"
      },
      "source": [
        "ctm = CombinedTM(input_size=len(tp.vocab), bert_input_size=768, num_epochs=100, n_components=50)\n",
        "\n",
        "ctm.load(\"contextualized_topic_model_nc_50_tpm_0.0_tpv_0.98_hs_prodLDA_ac_(100, 100)_do_softplus_lr_0.2_mo_0.002_rp_0.99/\",\n",
        "                                                                                                      epoch=26)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SEBG6wj9Zdu"
      },
      "source": [
        "# Topics\n",
        "\n",
        "After training, now it is the time to look at our topics: we can use the \n",
        "\n",
        "```\n",
        "get_topic_lists\n",
        "```\n",
        "\n",
        "function to get the topics. It also accept a parameter that allows you to select how many words you want to see for each topic.\n",
        "\n",
        "If you look at the topics, you will see that they all make sense and are representative of a collection of documents that comes from Wikipedia (general knowledge)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxcKgjbx3V2o"
      },
      "source": [
        "ctm.get_topic_lists(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0EB30NeuGmJ"
      },
      "source": [
        "## Using the TestSet\n",
        "\n",
        "Now we are going to use the testset: we want to predict the topic for unseen documents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "On9saaijuKcV"
      },
      "source": [
        "testing_dataset = tp.create_test_set(testing_contextual_documents, testing_bow_documents) # create dataset for the testset\n",
        "predictions = ctm.get_doc_topic_distribution(testing_dataset, n_samples=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xEn4nTCvuoG"
      },
      "source": [
        "print(testing_contextual_documents[15])\n",
        "\n",
        "topic_index = np.argmax(predictions[15])\n",
        "ctm.get_topic_lists(5)[topic_index]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}